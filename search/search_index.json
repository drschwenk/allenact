{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An open source Framework for Reproducible, Reusable, and Robust Embodied-AI Research. AllenAct is a modular and flexible learning framework designed with a focus on the unique requirements of Embodied-AI research. It provides first-class support for a growing collection of embodied environments, tasks and algorithms, provides reproductions of state-of-the-art models and includes extensive documentation, tutorials, start-up code, and pre-trained models. Quick Links # Website Docs Github Install Tutorials Citation Features Highlights # Support for multiple environments : Support for the i-THOR , Robo-THOR and Habitat embodied environments as well as for grid-worlds including MiniGrid . Task Abstraction : Tasks and environments are decoupled in AllenAct, enabling researchers to easily implement a large variety of tasks in the same environment. Algorithms : Support for a variety of on-policy algorithms including PPO , DD-PPO , A2C , Imitation Learning and DAgger as well as offline training such as offline IL. Sequential Algorithms : Trivial to experiment with different sequences of training routines, which are often the key to successful policies. Simultaneous Losses : Easily combine various losses while training models (e.g. use an external self-supervised loss while optimizing a PPO loss). Multi-agent support : Support for multi-agent algorithms and tasks. Visualizations : Out of the box support to easily visualize first and third person views for agents as well as intermediate model tensors, integrated into Tensorboard. Pre-trained models : Code and models for a number of standard Embodied AI tasks. Tutorials : Start-up code and extensive tutorials to help ramp up to Embodied AI. First-class PyTorch support : One of the few RL frameworks to target PyTorch. Contributions # We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in this repository is subject to formatting, documentation, and type-annotation guidelines. For more details, please see the our contribution guidelines . Acknowledgments # This work builds upon the pytorch-a2c-ppo-acktr library of Ilya Kostrikov and uses some data structures from FAIR's habitat-api . License # AllenAct is MIT licensed, as found in the LICENSE file. Team # AllenAct is an open-source project built by members of the PRIOR research group at the Allen Institute for Artificial Intelligence (AI2). AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. Citation # If you use this work, please cite: @article { AllenAct , author = {Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi} , title = {AllenAct: A Framework for Reproducible, Reusable, and Robust Embodied AI Research} , year = {2020} , journal = {arXiv} , }","title":"Overview"},{"location":"#quick-links","text":"Website Docs Github Install Tutorials Citation","title":"Quick Links"},{"location":"#features-highlights","text":"Support for multiple environments : Support for the i-THOR , Robo-THOR and Habitat embodied environments as well as for grid-worlds including MiniGrid . Task Abstraction : Tasks and environments are decoupled in AllenAct, enabling researchers to easily implement a large variety of tasks in the same environment. Algorithms : Support for a variety of on-policy algorithms including PPO , DD-PPO , A2C , Imitation Learning and DAgger as well as offline training such as offline IL. Sequential Algorithms : Trivial to experiment with different sequences of training routines, which are often the key to successful policies. Simultaneous Losses : Easily combine various losses while training models (e.g. use an external self-supervised loss while optimizing a PPO loss). Multi-agent support : Support for multi-agent algorithms and tasks. Visualizations : Out of the box support to easily visualize first and third person views for agents as well as intermediate model tensors, integrated into Tensorboard. Pre-trained models : Code and models for a number of standard Embodied AI tasks. Tutorials : Start-up code and extensive tutorials to help ramp up to Embodied AI. First-class PyTorch support : One of the few RL frameworks to target PyTorch.","title":"Features &amp; Highlights"},{"location":"#contributions","text":"We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in this repository is subject to formatting, documentation, and type-annotation guidelines. For more details, please see the our contribution guidelines .","title":"Contributions"},{"location":"#acknowledgments","text":"This work builds upon the pytorch-a2c-ppo-acktr library of Ilya Kostrikov and uses some data structures from FAIR's habitat-api .","title":"Acknowledgments"},{"location":"#license","text":"AllenAct is MIT licensed, as found in the LICENSE file.","title":"License"},{"location":"#team","text":"AllenAct is an open-source project built by members of the PRIOR research group at the Allen Institute for Artificial Intelligence (AI2). AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.","title":"Team"},{"location":"#citation","text":"If you use this work, please cite: @article { AllenAct , author = {Luca Weihs and Jordi Salvador and Klemen Kotar and Unnat Jain and Kuo-Hao Zeng and Roozbeh Mottaghi and Aniruddha Kembhavi} , title = {AllenAct: A Framework for Reproducible, Reusable, and Robust Embodied AI Research} , year = {2020} , journal = {arXiv} , }","title":"Citation"},{"location":"CONTRIBUTING/","text":"Contributing # We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in pull requests should adhere to the following guidelines. Found a bug or want to suggest an enhancement? # Please submit an issue in which you note the steps to reproduce the bug or in which you detail the enhancement. Making a pull request? # When making a pull request we require that any code respects several guidelines detailed below. Auto-formatting # All python code in this repository should be formatted using black . To use black auto-formatting across all files, simply run bash scripts/auto_format.sh which will run black auto-formatting as well as docformatter (used to auto-format documentation strings). Type-checking # Our code makes liberal use of type hints. If you have not had experience with type hinting in python we recommend reading the documentation of the typing python module or the simplified introduction to type hints found here . All methods should have typed arguments and output. Furthermore we use mypy to perform basic static type checking. Before making a pull request, there should be no warnings or errors when running dmypy run -- --follow-imports = skip . Explicitly ignoring type checking (for instance using # type: ignore ) should be only be done when it would otherwise be an extensive burden. Updating, adding, or removing packages? # We recommend using pipenv to keep track of dependencies, ensure reproducibility, and keep things synchronized. If you are doing so and have modified any installed packages please run: pipenv-setup sync --pipfile # Syncs packages to setup.py pip freeze requirements.txt # Syncs packages to requirements.py before submitting a pull request. If you are not using pipenv , you are still required to update the file Pipfile with newly installed or modified packages. Moreover you must manually update the install_requires field of the setup.py file. Setting up pre-commit hooks (optional) # Pre-commit hooks check that, when you attempt to commit changes, your code adheres a number of formatting and type-checking guidelines. Pull requests containing code not adhering to these guidelines will not be accepted and thus we recommend installing these pre-commit hooks. Assuming you have installed all of the project requirements, you can install our recommended pre-commit hooks by running (from this project's root directory) pre-commit install After running the above, each time you run git commit ... a set of pre-commit checks will be run.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We welcome contributions from the greater community. If you would like to make such a contributions we recommend first submitting an issue describing your proposed improvement. Doing so can ensure we can validate your suggestions before you spend a great deal of time upon them. Improvements and bug fixes should be made via a pull request from your fork of the repository at https://github.com/allenai/allenact . All code in pull requests should adhere to the following guidelines.","title":"Contributing"},{"location":"CONTRIBUTING/#found-a-bug-or-want-to-suggest-an-enhancement","text":"Please submit an issue in which you note the steps to reproduce the bug or in which you detail the enhancement.","title":"Found a bug or want to suggest an enhancement?"},{"location":"CONTRIBUTING/#making-a-pull-request","text":"When making a pull request we require that any code respects several guidelines detailed below.","title":"Making a pull request?"},{"location":"CONTRIBUTING/#auto-formatting","text":"All python code in this repository should be formatted using black . To use black auto-formatting across all files, simply run bash scripts/auto_format.sh which will run black auto-formatting as well as docformatter (used to auto-format documentation strings).","title":"Auto-formatting"},{"location":"CONTRIBUTING/#type-checking","text":"Our code makes liberal use of type hints. If you have not had experience with type hinting in python we recommend reading the documentation of the typing python module or the simplified introduction to type hints found here . All methods should have typed arguments and output. Furthermore we use mypy to perform basic static type checking. Before making a pull request, there should be no warnings or errors when running dmypy run -- --follow-imports = skip . Explicitly ignoring type checking (for instance using # type: ignore ) should be only be done when it would otherwise be an extensive burden.","title":"Type-checking"},{"location":"CONTRIBUTING/#updating-adding-or-removing-packages","text":"We recommend using pipenv to keep track of dependencies, ensure reproducibility, and keep things synchronized. If you are doing so and have modified any installed packages please run: pipenv-setup sync --pipfile # Syncs packages to setup.py pip freeze requirements.txt # Syncs packages to requirements.py before submitting a pull request. If you are not using pipenv , you are still required to update the file Pipfile with newly installed or modified packages. Moreover you must manually update the install_requires field of the setup.py file.","title":"Updating, adding, or removing packages?"},{"location":"CONTRIBUTING/#setting-up-pre-commit-hooks-optional","text":"Pre-commit hooks check that, when you attempt to commit changes, your code adheres a number of formatting and type-checking guidelines. Pull requests containing code not adhering to these guidelines will not be accepted and thus we recommend installing these pre-commit hooks. Assuming you have installed all of the project requirements, you can install our recommended pre-commit hooks by running (from this project's root directory) pre-commit install After running the above, each time you run git commit ... a set of pre-commit checks will be run.","title":"Setting up pre-commit hooks (optional)"},{"location":"FAQ/","text":"FAQ # How do I generate documentation? # Documentation is generated using mkdoc and pydoc-markdown . Building documentation locally # If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build Serving documentation locally # If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/ Modifying and serving documentation locally # If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#how-do-i-generate-documentation","text":"Documentation is generated using mkdoc and pydoc-markdown .","title":"How do I generate documentation?"},{"location":"FAQ/#building-documentation-locally","text":"If you have made no changes to the documentation and only wish to build documentation on your local machine, run the following from within the allenact root directory. Note: This will generate HTML documentation within the site folder mkdocs build","title":"Building documentation locally"},{"location":"FAQ/#serving-documentation-locally","text":"If you have made no changes to the documentation and only wish to serve documentation on your local machine (with live reloading of modified documentation), run the following from within the allenact root directory. mkdocs serve Then navigate to http://127.0.0.1:8000/","title":"Serving documentation locally"},{"location":"FAQ/#modifying-and-serving-documentation-locally","text":"If you have made changes to the documentation, you will need to run a documentation builder script before you serve it on your local machine. bash scripts/build_docs.sh mkdocs serve Then navigate to http://127.0.0.1:8000/ Alternatively, the site directory (once built) can be served as a static webpage on your local machine without installing any dependencies by running python -m http.server 8000 from within the site directory.","title":"Modifying and serving documentation locally"},{"location":"LICENSE/","text":"MIT License Original work Copyright (c) 2017 Ilya Kostrikov Original work Copyright (c) Facebook, Inc. and its affiliates. Modified work Copyright (c) 2020 Allen Institute for Artificial Intelligence Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licence"},{"location":"getting_started/abstractions/","text":"TODO: links to API should be fixed # Primary abstractions # Our package relies on a collection of fundamental abstractions to define how, and in what task, the model should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstract (if relevant). The following provides a high-level description of how these abstractions interact. Task # Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNavTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation . Sensor # Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation . Task sampler # A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation . Actor critic model # The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation . Actor critic loss # Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the abstract AbstractActorCriticLoss class and an example implementation . Experiment configuration # In allenact , experiments are definied by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation .","title":"Primary abstractions"},{"location":"getting_started/abstractions/#todo-links-to-api-should-be-fixed","text":"","title":"TODO: links to API should be fixed"},{"location":"getting_started/abstractions/#primary-abstractions","text":"Our package relies on a collection of fundamental abstractions to define how, and in what task, the model should be trained and evaluated. A subset of these abstractions are described in plain language below. Each of the below sections end with a link to the (formal) documentation of the abstraction as well as a link to an example implementation of the abstract (if relevant). The following provides a high-level description of how these abstractions interact.","title":"Primary abstractions"},{"location":"getting_started/abstractions/#task","text":"Tasks define the scope of the interaction between agents and an environment (including the action types agents are allowed to execute), as well as metrics to evaluate the agents' performance. For example, we might define a task ObjectNavTask in which agents receive observations obtained from the environment (e.g. RGB images) or directly from the task (e.g. a target object class) and are allowed to execute actions such as MoveAhead , RotateRight , RotateLeft , and End whenever agents determine they have reached their target. The metrics might include a success indicator or some quantitative metric on the optimality of the followed path. See the abstract Task class and an example implementation .","title":"Task"},{"location":"getting_started/abstractions/#sensor","text":"Sensors provide observations extracted from an environment (e.g. RGB or depth images) or directly from a task (e.g. the end point in point navigation or target object class in semantic navigation) that can be directly consumed by agents. See the abstract Sensor class and an example implementation .","title":"Sensor"},{"location":"getting_started/abstractions/#task-sampler","text":"A task sampler is responsible for generating a sequence of tasks for agents to solve. The sequence of tasks can be randomly generated (e.g. in training) or extracted from an ordered pool (e.g. in validation or testing). See the abstract TaskSampler class and an example implementation .","title":"Task sampler"},{"location":"getting_started/abstractions/#actor-critic-model","text":"The actor-critic agent is responsible for computing batched action probabilities and state values given the observations provided by sensors, internal state representations, previous actions, and potentially other inputs. See the abstract ActorCriticModel class and an example implementation .","title":"Actor critic model"},{"location":"getting_started/abstractions/#actor-critic-loss","text":"Actor-critic losses compute a combination of action loss and value loss out of collected experience that can be used to train actor-critic models with back-propagation, e.g. PPO or A2C. See the abstract AbstractActorCriticLoss class and an example implementation .","title":"Actor critic loss"},{"location":"getting_started/abstractions/#experiment-configuration","text":"In allenact , experiments are definied by implementing the abstract ExperimentConfig class. The methods of this implementation are then called during training/inference to properly set up the desired experiment. For example, the ExperimentConfig.create_model method will be called at the beginning of training to create the model to be trained. See either the \"designing your first minigrid experiment\" or the \"designing an experiment for point navigation\" tutorials to get an in-depth description of how these experiment configurations are defined in practice. See also the abstract ExperimentConfig class and an example implementation .","title":"Experiment configuration"},{"location":"getting_started/running-your-first-experiment/","text":"Running your first experiment # TODO: replace the following with a simple experiment that finishes in 2-3 minutes # Assuming you have installed all of the requirements , you can run your first experiment by calling python ddmain.py object_nav_thor_ppo -s 12345 This runs the experiment defined in experiments/object_nav_thor_ppo.py with seed 12345. If everything was installed correctly, a simple semantic navigation model for AI2-THOR will be trained and validated and a new folder experiment_output will be created containing a checkpoints/LOCAL_TIME_STR/ subfolder with different checkpoints, a used_configs/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/ObjectNavPPO/LOCAL_TIME_STR/ . To run your own custom experiment simply define a new experiment configuration in a file experiments/my_custom_experiment.py after which you may run it with python ddmain.py my_custom_experiment .","title":"Run your first experiment"},{"location":"getting_started/running-your-first-experiment/#running-your-first-experiment","text":"","title":"Running your first experiment"},{"location":"getting_started/running-your-first-experiment/#todo-replace-the-following-with-a-simple-experiment-that-finishes-in-2-3-minutes","text":"Assuming you have installed all of the requirements , you can run your first experiment by calling python ddmain.py object_nav_thor_ppo -s 12345 This runs the experiment defined in experiments/object_nav_thor_ppo.py with seed 12345. If everything was installed correctly, a simple semantic navigation model for AI2-THOR will be trained and validated and a new folder experiment_output will be created containing a checkpoints/LOCAL_TIME_STR/ subfolder with different checkpoints, a used_configs/LOCAL_TIME_STR/ subfolder with all used configuration files, and a tensorboard log file under tb/ObjectNavPPO/LOCAL_TIME_STR/ . To run your own custom experiment simply define a new experiment configuration in a file experiments/my_custom_experiment.py after which you may run it with python ddmain.py my_custom_experiment .","title":"TODO: replace the following with a simple experiment that finishes in 2-3 minutes"},{"location":"getting_started/structure/","text":"Structure of the codebase # The codebase consists of the following directories: docs , plugins , projects , scripts , common , tests and utils . Below, we explain the overall structure and how different components of the codebase are organized. docs TODO # plugins TODO # projects TODO # scripts TODO # common TODO # tests TODO # utils TODO #","title":"Structure of the codebase"},{"location":"getting_started/structure/#structure-of-the-codebase","text":"The codebase consists of the following directories: docs , plugins , projects , scripts , common , tests and utils . Below, we explain the overall structure and how different components of the codebase are organized.","title":"Structure of the codebase"},{"location":"getting_started/structure/#docs-todo","text":"","title":"docs TODO"},{"location":"getting_started/structure/#plugins-todo","text":"","title":"plugins TODO"},{"location":"getting_started/structure/#projects-todo","text":"","title":"projects TODO"},{"location":"getting_started/structure/#scripts-todo","text":"","title":"scripts TODO"},{"location":"getting_started/structure/#common-todo","text":"","title":"common TODO"},{"location":"getting_started/structure/#tests-todo","text":"","title":"tests TODO"},{"location":"getting_started/structure/#utils-todo","text":"","title":"utils TODO"},{"location":"howtos/changing-rewards-and-losses/","text":"Changing rewards and losses # In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level. Rewards # We will use the object navigation task in AI2-THOR as a running example. We can see how the ObjectNavTask._step(self, action: int) - RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) - float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( rl_ai2thor . object_nav . tasks . ObjectNavTask ): def judge ( self ) - float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward Losses # Currently we support A2C , PPO , and imitation losses. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return utils . experiment_utils . TrainingPipeline ( named_losses = { imitation_loss : utils . experiment_utils . Builder ( onpolicy_sync . losses . imitation . Imitation , ), ppo_loss : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ imitation_loss , ppo_loss ], teacher_forcing = utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), utils . experiment_utils . PipelineStage ( loss_names = [ ppo_loss , imitation_loss ], max_stage_steps = ppo_steps ), utils . experiment_utils . PipelineStage ( loss_names = [ ppo_loss ], max_stage_steps = ppo_steps2 , ), ], )","title":"Change rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#changing-rewards-and-losses","text":"In order to train actor-critic agents, we need to specify rewards at the task level, and losses at the training pipeline level.","title":"Changing rewards and losses"},{"location":"howtos/changing-rewards-and-losses/#rewards","text":"We will use the object navigation task in AI2-THOR as a running example. We can see how the ObjectNavTask._step(self, action: int) - RLStepResult method computes the reward for the latest action by invoking a function like: def judge ( self ) - float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Any reward shaping can be easily added by e.g. modifying the definition of an existing class: class NavigationWithShaping ( rl_ai2thor . object_nav . tasks . ObjectNavTask ): def judge ( self ) - float : reward = super () . judge () if self . previous_state is not None : reward += float ( my_reward_shaping_function ( self . previous_state , self . current_state , )) self . previous_state = self . current_state return reward","title":"Rewards"},{"location":"howtos/changing-rewards-and-losses/#losses","text":"Currently we support A2C , PPO , and imitation losses. We can easily include DAgger or variations thereof by assuming the availability of an expert providing optimal actions to agents and combining imitation and PPO losses in different ways through multiple stages: class MyExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) ppo_steps = int ( 3e4 ) ppo_steps2 = int ( 1e6 ) ... return utils . experiment_utils . TrainingPipeline ( named_losses = { imitation_loss : utils . experiment_utils . Builder ( onpolicy_sync . losses . imitation . Imitation , ), ppo_loss : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, ... pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ imitation_loss , ppo_loss ], teacher_forcing = utils . experiment_utils . LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), utils . experiment_utils . PipelineStage ( loss_names = [ ppo_loss , imitation_loss ], max_stage_steps = ppo_steps ), utils . experiment_utils . PipelineStage ( loss_names = [ ppo_loss ], max_stage_steps = ppo_steps2 , ), ], )","title":"Losses"},{"location":"howtos/defining-a-new-model/","text":"Defining a new model # All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_hidden_state_size , returning the size of the model's hidden state; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example. Actor-critic model interface # As an example, let's build an object navigation agent. class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , ): super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( self . observation_space , hidden_size ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . recurrent_hidden_state_size ) + object_type_embedding_dim , self . recurrent_hidden_state_size , ) self . actor = LinearActorHead ( self . recurrent_hidden_state_size , action_space . n ) self . critic = LinearCriticHead ( self . recurrent_hidden_state_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ): return self . _hidden_size def forward ( self , observations , rnn_hidden_states , prev_actions , masks ): target_encoding = self . object_type_embedding ( observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x = torch . cat ( x , dim = 1 ) x , rnn_hidden_states = self . state_encoder ( x , rnn_hidden_states , masks ) return ( ActorCriticOutput ( distributions = self . actor ( x ), values = self . critic ( x ), extras = {} ), rnn_hidden_states , ) ... On-policy RL engine requirements # Apart from the interface expected by all actor-critic models, we also need to provide a utility function to allow the on-policy RL engine to properly initalize the rollouts storage, num_recurrent_layers : class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): ... @property def num_recurrent_layers ( self ): return self . state_encoder . num_recurrent_layers","title":"Define a new model"},{"location":"howtos/defining-a-new-model/#defining-a-new-model","text":"All actor-critic models must implement the interface described by the ActorCriticModel class . This interface includes two methods that need to be implemented: recurrent_hidden_state_size , returning the size of the model's hidden state; and forward , returning an ActorCriticOutput given the current observation, hidden state and previous actions. For convenience, we provide a recurrent network module and a simple CNN module from the Habitat baseline navigation models, that will be used in this example.","title":"Defining a new model"},{"location":"howtos/defining-a-new-model/#actor-critic-model-interface","text":"As an example, let's build an object navigation agent. class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): def __init__ ( self , action_space : gym . spaces . Discrete , observation_space : SpaceDict , goal_sensor_uuid : str , hidden_size = 512 , object_type_embedding_dim = 8 , ): super () . __init__ ( action_space = action_space , observation_space = observation_space ) self . goal_sensor_uuid = goal_sensor_uuid self . _n_object_types = self . observation_space . spaces [ self . goal_sensor_uuid ] . n self . _hidden_size = hidden_size self . object_type_embedding_size = object_type_embedding_dim self . visual_encoder = SimpleCNN ( self . observation_space , hidden_size ) self . state_encoder = RNNStateEncoder ( ( 0 if self . is_blind else self . recurrent_hidden_state_size ) + object_type_embedding_dim , self . recurrent_hidden_state_size , ) self . actor = LinearActorHead ( self . recurrent_hidden_state_size , action_space . n ) self . critic = LinearCriticHead ( self . recurrent_hidden_state_size ) self . object_type_embedding = nn . Embedding ( num_embeddings = self . _n_object_types , embedding_dim = object_type_embedding_dim , ) self . train () @property def recurrent_hidden_state_size ( self ): return self . _hidden_size def forward ( self , observations , rnn_hidden_states , prev_actions , masks ): target_encoding = self . object_type_embedding ( observations [ self . goal_sensor_uuid ] . to ( torch . int64 ) ) x = [ target_encoding ] if not self . is_blind : perception_embed = self . visual_encoder ( observations ) x = [ perception_embed ] + x x = torch . cat ( x , dim = 1 ) x , rnn_hidden_states = self . state_encoder ( x , rnn_hidden_states , masks ) return ( ActorCriticOutput ( distributions = self . actor ( x ), values = self . critic ( x ), extras = {} ), rnn_hidden_states , ) ...","title":"Actor-critic model interface"},{"location":"howtos/defining-a-new-model/#on-policy-rl-engine-requirements","text":"Apart from the interface expected by all actor-critic models, we also need to provide a utility function to allow the on-policy RL engine to properly initalize the rollouts storage, num_recurrent_layers : class ObjectNavActorCritic ( ActorCriticModel [ CategoricalDistr ]): ... @property def num_recurrent_layers ( self ): return self . state_encoder . num_recurrent_layers","title":"On-policy RL engine requirements"},{"location":"howtos/defining-a-new-task/","text":"Defining a new task # In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing. Task # Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition. Initialization, action space and termination # Let's start with the definition of the action space and task initialization: class ObjectNavTask ( Task [ IThorEnvironment ]): _actions = ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) - None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) - Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) - bool : return self . _took_end_action def close ( self ) - None : self . env . stop () ... Step method # Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNavTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : int ) - RLStepResult : action_str = self . class_action_names ()[ action ] if action_str == END : self . _took_end_action = True self . _success = self . _is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ action : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { last_action_success : self . last_action_success }, ) return step_result def _is_goal_object_visible ( self ) - bool : return any ( o [ objectType ] == self . task_info [ object_type ] for o in self . env . visible_objects () ) def judge ( self ) - float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward ) Metrics, rendering and expert actions # Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = rgb , * args , ** kwargs ) - numpy . ndarray : assert mode == rgb , only rgb rendering is implemented return self . env . current_frame def metrics ( self ) - Dict [ str , Any ]: if not self . is_done (): return {} else : return { success : self . _success , ep_length : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) - Tuple [ int , bool ]: return my_objnav_expert_implementation ( self ) TaskSampler # We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR. Initialization and termination # class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) - None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNavTask ] = None set_seed ( seed ) self . reset () def close ( self ) - None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) - IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env Task sampling # Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) - Union [ int , float ]: return float ( inf ) @property def total_unique ( self ) - Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) - Optional [ ObjectNavTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) - bool : return True def next_task ( self ) - Optional [ ObjectNavTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () object_types_in_scene = set ( [ o [ objectType ] for o in self . env . last_event . metadata [ objects ]] ) task_info = { object_type : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNavTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Define a new task"},{"location":"howtos/defining-a-new-task/#defining-a-new-task","text":"In order to use new tasks in our experiments, we need to define two classes: A Task , including, among others, a step implementation providing a RLStepResult , a metrics method providing quantitative performance measurements for agents and, optionally, a query_expert method that can be used e.g. with an imitation loss during training. A TaskSampler , that allows instantiating new Tasks for the agents to solve during training, validation and testing.","title":"Defining a new task"},{"location":"howtos/defining-a-new-task/#task","text":"Let's define a semantic navigation task, where agents have to navigate from a starting point in an environment to an object of a specific class using a minimal amount of steps and deciding when the goal has been reached. We need to define the methods action_space , render , _step , reached_terminal_state , class_action_names , close , metrics , and query_expert from the base Task definition.","title":"Task"},{"location":"howtos/defining-a-new-task/#initialization-action-space-and-termination","text":"Let's start with the definition of the action space and task initialization: class ObjectNavTask ( Task [ IThorEnvironment ]): _actions = ( MOVE_AHEAD , ROTATE_LEFT , ROTATE_RIGHT , LOOK_DOWN , LOOK_UP , END ) def __init__ ( self , env : IThorEnvironment , sensors : List [ Sensor ], task_info : Dict [ str , Any ], max_steps : int , ** kwargs ) - None : super () . __init__ ( env = env , sensors = sensors , task_info = task_info , max_steps = max_steps , ** kwargs ) self . _took_end_action : bool = False self . _success : Optional [ bool ] = False @property def action_space ( self ): return gym . spaces . Discrete ( len ( self . _actions )) @classmethod def class_action_names ( cls ) - Tuple [ str , ... ]: return cls . _actions def reached_terminal_state ( self ) - bool : return self . _took_end_action def close ( self ) - None : self . env . stop () ...","title":"Initialization, action space and termination"},{"location":"howtos/defining-a-new-task/#step-method","text":"Next, we define the main method _step that will be called every time the agent produces a new action: class ObjectNavTask ( Task [ IThorEnvironment ]): ... def _step ( self , action : int ) - RLStepResult : action_str = self . class_action_names ()[ action ] if action_str == END : self . _took_end_action = True self . _success = self . _is_goal_object_visible () self . last_action_success = self . _success else : self . env . step ({ action : action_str }) self . last_action_success = self . env . last_action_success step_result = RLStepResult ( observation = self . get_observations (), reward = self . judge (), done = self . is_done (), info = { last_action_success : self . last_action_success }, ) return step_result def _is_goal_object_visible ( self ) - bool : return any ( o [ objectType ] == self . task_info [ object_type ] for o in self . env . visible_objects () ) def judge ( self ) - float : reward = - 0.01 if not self . last_action_success : reward += - 0.03 if self . _took_end_action : reward += 1.0 if self . _success else - 1.0 return float ( reward )","title":"Step method"},{"location":"howtos/defining-a-new-task/#metrics-rendering-and-expert-actions","text":"Finally, we define methods to render and evaluate the current task, and optionally generate expert actions to be used e.g. for DAgger training. def render ( self , mode : str = rgb , * args , ** kwargs ) - numpy . ndarray : assert mode == rgb , only rgb rendering is implemented return self . env . current_frame def metrics ( self ) - Dict [ str , Any ]: if not self . is_done (): return {} else : return { success : self . _success , ep_length : self . num_steps_taken ()} def query_expert ( self , ** kwargs ) - Tuple [ int , bool ]: return my_objnav_expert_implementation ( self )","title":"Metrics, rendering and expert actions"},{"location":"howtos/defining-a-new-task/#tasksampler","text":"We also need to define the corresponding TaskSampler, which must contain implementations for methods __len__ , total_unique , last_sampled_task , next_task , close , reset , and set_seed . Currently, an additional method all_observation_spaces_equal is used to ensure compatibility with the current RolloutStorage . Let's define a tasks sampler able to provide an infinite number of object navigation tasks for AI2-THOR.","title":"TaskSampler"},{"location":"howtos/defining-a-new-task/#initialization-and-termination","text":"class ObjectNavTaskSampler ( TaskSampler ): def __init__ ( self , scenes : List [ str ], object_types : str , sensors : List [ Sensor ], max_steps : int , env_args : Dict [ str , Any ], action_space : gym . Space , seed : Optional [ int ] = None , deterministic_cudnn : bool = False , * args , ** kwargs ) - None : self . env_args = env_args self . scenes = scenes self . object_types = object_types self . grid_size = 0.25 self . env : Optional [ IThorEnvironment ] = None self . sensors = sensors self . max_steps = max_steps self . _action_sapce = action_space self . scene_id : Optional [ int ] = None self . _last_sampled_task : Optional [ ObjectNavTask ] = None set_seed ( seed ) self . reset () def close ( self ) - None : if self . env is not None : self . env . stop () def reset ( self ): self . scene_id = 0 def _create_environment ( self ) - IThorEnvironment : env = IThorEnvironment ( make_agents_visible = False , object_open_speed = 0.05 , restrict_to_initially_reachable_points = True , ** self . env_args , ) return env","title":"Initialization and termination"},{"location":"howtos/defining-a-new-task/#task-sampling","text":"Finally, we need to define methods to determine the number of available tasks (possibly infinite) and sample tasks: @property def length ( self ) - Union [ int , float ]: return float ( inf ) @property def total_unique ( self ) - Optional [ Union [ int , float ]]: return None @property def last_sampled_task ( self ) - Optional [ ObjectNavTask ]: return self . _last_sampled_task @property def all_observation_spaces_equal ( self ) - bool : return True def next_task ( self ) - Optional [ ObjectNavTask ]: self . scene_id = random . randint ( 0 , len ( self . scenes ) - 1 ) self . scene = self . scenes [ self . scene_id ] if self . env is not None : if scene != self . env . scene_name : self . env . reset ( scene ) else : self . env = self . _create_environment () self . env . reset ( scene_name = scene ) self . env . randomize_agent_location () object_types_in_scene = set ( [ o [ objectType ] for o in self . env . last_event . metadata [ objects ]] ) task_info = { object_type : random . sample ( self . object_types , 1 )} self . _last_sampled_task = ObjectNavTask ( env = self . env , sensors = self . sensors , task_info = task_info , max_steps = self . max_steps , action_space = self . _action_sapce , ) return self . _last_sampled_task","title":"Task sampling"},{"location":"howtos/defining-a-new-training-pipeline/","text":"To-do #","title":"Define a new training pipeline"},{"location":"howtos/defining-a-new-training-pipeline/#to-do","text":"","title":"To-do"},{"location":"howtos/defining-an-experiment/","text":"Defining an experiment # Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. The interface to be implemented by the experiment specification is defined in rl_base.experiment_config . The first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): @classmethod def tag ( cls ): return ObjectNavThorPPO ... Model creation # Next, create_model will be used to instantiate object navigation baseline actor-critic models : class ObjectNavThorExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... SCREEN_SIZE = 224 ... OBJECT_TYPES = sorted ([ Tomato ]) ... SENSORS = [ rl_ai2thor . ai2thor_sensors . RGBSensorThor ( { height : SCREEN_SIZE , width : SCREEN_SIZE , use_resnet_normalization : True , } ), rl_ai2thor . ai2thor_sensors . GoalObjectTypeThorSensor ( { object_types : OBJECT_TYPES } ), ] @classmethod def create_model ( cls , ** kwargs ) - torch . nn . Module : return models . object_nav_models . ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( rl_ai2thor . object_nav . tasks . ObjectNavTask . class_action_names ()) ), observation_space = rl_base . sensor . SensorSuite ( cls . SENSORS ) . observation_spaces , goal_sensor_uuid = goal_object_type_ind , hidden_size = 512 , object_type_embedding_dim = 8 , ) ... Training pipeline # In this section we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return utils . experiment_utils . TrainingPipeline ( named_losses = { ppo_loss : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, optimizer = utils . experiment_utils . Builder ( torch . optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks num_mini_batch = 1 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ ppo_loss ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complicated pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( { height : SCREEN_SIZE , width : SCREEN_SIZE , use_resnet_normalization : True , } ), GoalObjectTypeThorSensor ({ object_types : OBJECT_TYPES }), ExpertActionSensor ({ nactions : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks optimizer = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 if not torch . cuda . is_available () else 30 , update_repeats = 4 , num_steps = 128 , named_losses = { imitation_loss : Builder ( Imitation ,), # We add an imitation loss. ppo_loss : Builder ( PPO , default = PPOConfig ,), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ imitation_loss ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ ppo_loss , imitation_loss ], max_stage_steps = int ( 3e4 ) ), ], ) Note that, in order for the saved configs in the experiment output folder to be fully usable, we currently need to import the module with the parent experiment config relative to the current location. Machine configuration # In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = train , ** kwargs ): on_server = torch . cuda . is_available () if mode == train : nprocesses = 6 if not on_server else 20 gpu_ids = [] if not on_server else [ 0 ] elif mode == valid : nprocesses = 0 gpu_ids = [] if not on_server else [ 1 ] elif mode == test : nprocesses = 1 gpu_ids = [] if not on_server else [ 0 ] else : raise NotImplementedError ( mode must be train , valid , or test . ) return { nprocesses : nprocesses , gpu_ids : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.is_available() ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers. Task sampling # The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) - rl_base . task . TaskSampler : return rl_ai2thor . object_nav . task_samplers . ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : typing . Optional [ typing . List [ int ]] = None , seeds : typing . Optional [ typing . List [ int ]] = None , deterministic_cudnn : bool = False , ) - typing . Dict [ str , typing . Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ scene_period ] = self . SCENE_PERIOD res [ env_args ][ x_display ] = None return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information.","title":"Define an experiment"},{"location":"howtos/defining-an-experiment/#defining-an-experiment","text":"Let's look at an example experiment configuration for an object navigation example with an actor-critic agent observing RGB images from the environment and target object classes from the task. The interface to be implemented by the experiment specification is defined in rl_base.experiment_config . The first method to implement is tag , which provides a string identifying the experiment: class ObjectNavThorPPOExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): @classmethod def tag ( cls ): return ObjectNavThorPPO ...","title":"Defining an  experiment"},{"location":"howtos/defining-an-experiment/#model-creation","text":"Next, create_model will be used to instantiate object navigation baseline actor-critic models : class ObjectNavThorExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... SCREEN_SIZE = 224 ... OBJECT_TYPES = sorted ([ Tomato ]) ... SENSORS = [ rl_ai2thor . ai2thor_sensors . RGBSensorThor ( { height : SCREEN_SIZE , width : SCREEN_SIZE , use_resnet_normalization : True , } ), rl_ai2thor . ai2thor_sensors . GoalObjectTypeThorSensor ( { object_types : OBJECT_TYPES } ), ] @classmethod def create_model ( cls , ** kwargs ) - torch . nn . Module : return models . object_nav_models . ObjectNavBaselineActorCritic ( action_space = gym . spaces . Discrete ( len ( rl_ai2thor . object_nav . tasks . ObjectNavTask . class_action_names ()) ), observation_space = rl_base . sensor . SensorSuite ( cls . SENSORS ) . observation_spaces , goal_sensor_uuid = goal_object_type_ind , hidden_size = 512 , object_type_embedding_dim = 8 , ) ...","title":"Model creation"},{"location":"howtos/defining-an-experiment/#training-pipeline","text":"In this section we use Builder objects, which allow us to defer the instantiation of objects of the class passed as their first argument while allowing passing additional keyword arguments to their initializers. We can implement a training pipeline which trains with a single stage using PPO: class ObjectNavThorPPOExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... @classmethod def training_pipeline ( cls , ** kwargs ): return utils . experiment_utils . TrainingPipeline ( named_losses = { ppo_loss : utils . experiment_utils . Builder ( onpolicy_sync . losses . ppo . PPO , default = onpolicy_sync . losses . ppo . PPOConfig , ), }, optimizer = utils . experiment_utils . Builder ( torch . optim . Adam , dict ( lr = 2.5e-4 ) ), save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks num_mini_batch = 1 , update_repeats = 4 , num_steps = 128 , gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ utils . experiment_utils . PipelineStage ( loss_names = [ ppo_loss ], max_stage_steps = int ( 1e6 ) ), ], ) ... Alternatively, we could use a more complicated pipeline that includes dataset aggregation ( DAgger ). This requires the existence of an expert (implemented in the task definition) that provides optimal actions to agents. We have implemented such a pipeline by extending the above configuration as follows class ObjectNavThorPPOExperimentConfig ( ObjectNavThorPPOExperimentConfig ): ... SENSORS = [ RGBSensorThor ( { height : SCREEN_SIZE , width : SCREEN_SIZE , use_resnet_normalization : True , } ), GoalObjectTypeThorSensor ({ object_types : OBJECT_TYPES }), ExpertActionSensor ({ nactions : 6 }), # Notice that we have added # an expert action sensor. ] ... @classmethod def training_pipeline ( cls , ** kwargs ): dagger_steps = int ( 3e4 ) return TrainingPipeline ( save_interval = 10000 , # Save every 10000 steps (approximately) metric_accumulate_interval = cls . MAX_STEPS * 10 , # Log every 10 max length tasks optimizer = Builder ( optim . Adam , dict ( lr = 2.5e-4 )), num_mini_batch = 6 if not torch . cuda . is_available () else 30 , update_repeats = 4 , num_steps = 128 , named_losses = { imitation_loss : Builder ( Imitation ,), # We add an imitation loss. ppo_loss : Builder ( PPO , default = PPOConfig ,), }, gamma = 0.99 , use_gae = True , gae_lambda = 1.0 , max_grad_norm = 0.5 , pipeline_stages = [ # The pipeline now has two stages, in the first # we use DAgger (imitation loss + teacher forcing). # In the second stage we no longer use teacher # forcing and add in the ppo loss. PipelineStage ( loss_names = [ imitation_loss ], teacher_forcing = LinearDecay ( startp = 1.0 , endp = 0.0 , steps = dagger_steps , ), max_stage_steps = dagger_steps , ), PipelineStage ( loss_names = [ ppo_loss , imitation_loss ], max_stage_steps = int ( 3e4 ) ), ], ) Note that, in order for the saved configs in the experiment output folder to be fully usable, we currently need to import the module with the parent experiment config relative to the current location.","title":"Training pipeline"},{"location":"howtos/defining-an-experiment/#machine-configuration","text":"In machine_params we define machine configuration parameters that will be used for training, validation and test: class ObjectNavThorPPOExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... @classmethod def machine_params ( cls , mode = train , ** kwargs ): on_server = torch . cuda . is_available () if mode == train : nprocesses = 6 if not on_server else 20 gpu_ids = [] if not on_server else [ 0 ] elif mode == valid : nprocesses = 0 gpu_ids = [] if not on_server else [ 1 ] elif mode == test : nprocesses = 1 gpu_ids = [] if not on_server else [ 0 ] else : raise NotImplementedError ( mode must be train , valid , or test . ) return { nprocesses : nprocesses , gpu_ids : gpu_ids } ... In the above we use the availability of cuda ( torch.cuda.is_available() ) to determine whether we should use parameters appropriate for local machines or for a server. We might optionally add a list of sampler_devices to assign devices (likely those not used for running our agent) to task sampling workers.","title":"Machine configuration"},{"location":"howtos/defining-an-experiment/#task-sampling","text":"The above has defined the model we'd like to use, the types of losses we wish to use during training, and the machine specific parameters that should be used during training. Critically we have not yet defined which task we wish to train our agent to complete. This is done by implementing the ExperimentConfig.make_sampler_fn function class ObjectNavThorPPOExperimentConfig ( rl_base . experiment_config . ExperimentConfig ): ... @classmethod def make_sampler_fn ( cls , ** kwargs ) - rl_base . task . TaskSampler : return rl_ai2thor . object_nav . task_samplers . ObjectNavTaskSampler ( ** kwargs ) ... Now, before training starts, our trainer will know to generate a collection of task samplers using make_sampler_fn for training (and possibly validation or testing). The kwargs parameters used in the above function call can be different for each training process, we implement such differences using the ExperimentConfig.train_task_sampler_args function class ObjectNavThorPPOExperimentConfig ( ExperimentConfig ): ... def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : typing . Optional [ typing . List [ int ]] = None , seeds : typing . Optional [ typing . List [ int ]] = None , deterministic_cudnn : bool = False , ) - typing . Dict [ str , typing . Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_SCENES , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ scene_period ] = self . SCENE_PERIOD res [ env_args ][ x_display ] = None return res ... Now training process i out of n total processes will be instantiated with the parameters ObjectNavThorPPOExperimentConfig.train_task_sampler_args(i, n, ...) . Similar functions ( valid_task_sampler_args and test_task_sampler_args ) exist for generating validation and test parameters. Note also that with this function we can assign devices to run our environment for each worker. See the documentation of ExperimentConfig for more information.","title":"Task sampling"},{"location":"howtos/running-a-multi-agent-experiment/","text":"To-do #","title":"Run a multi-agent experiment"},{"location":"howtos/running-a-multi-agent-experiment/#to-do","text":"","title":"To-do"},{"location":"howtos/visualizing-results/","text":"To-do #","title":"Visualize results"},{"location":"howtos/visualizing-results/#to-do","text":"","title":"To-do"},{"location":"installation/installation-allenact/","text":"Installation of AllenAct # Clone the repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Note: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. In order to install requirements we recommend using pipenv but also include instructions if you would prefer to install things directly using pip . Installing requirements with pipenv ( recommended ) # If you have already installed pipenv , you may run the following to install all requirements. pipenv install --skip-lock --dev Installing requirements with pip # Note: do not run the following if you have already installed requirements with pipenv as above. If you prefer using pip , you may install all requirements as follows pip install -r requirements.txt Depending on your machine configuration, you may need to use pip3 instead of pip in the above.","title":"Installation of AllenAct"},{"location":"installation/installation-allenact/#installation-of-allenact","text":"Clone the repository to your local machine and move into the top-level directory git clone git@github.com:allenai/allenact.git cd allenact Note: This library has been tested only in python 3.6 . The following assumes you have a working version of python 3.6 installed locally. In order to install requirements we recommend using pipenv but also include instructions if you would prefer to install things directly using pip .","title":"Installation of AllenAct"},{"location":"installation/installation-allenact/#installing-requirements-with-pipenv-recommended","text":"If you have already installed pipenv , you may run the following to install all requirements. pipenv install --skip-lock --dev","title":"Installing requirements with pipenv (recommended)"},{"location":"installation/installation-allenact/#installing-requirements-with-pip","text":"Note: do not run the following if you have already installed requirements with pipenv as above. If you prefer using pip , you may install all requirements as follows pip install -r requirements.txt Depending on your machine configuration, you may need to use pip3 instead of pip in the above.","title":"Installing requirements with pip"},{"location":"installation/installation-framework/","text":"Installation of supported frameworks # Below we provide installation instruction for a number of frameworks that we support. Installation of AI2-THOR # Installation of Habitat # Installation of Minigrid #","title":"Installation of supported frameworks"},{"location":"installation/installation-framework/#installation-of-supported-frameworks","text":"Below we provide installation instruction for a number of frameworks that we support.","title":"Installation of supported frameworks"},{"location":"installation/installation-framework/#installation-of-ai2-thor","text":"","title":"Installation of AI2-THOR"},{"location":"installation/installation-framework/#installation-of-habitat","text":"","title":"Installation of Habitat"},{"location":"installation/installation-framework/#installation-of-minigrid","text":"","title":"Installation of Minigrid"},{"location":"notebooks/firstbook/","text":"To-do #","title":"To-do"},{"location":"notebooks/firstbook/#to-do","text":"","title":"To-do"},{"location":"projects/advisor_2020/","text":"Experiments for Advisor # TODO: # Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"Advisor code"},{"location":"projects/advisor_2020/#experiments-for-advisor","text":"","title":"Experiments for Advisor"},{"location":"projects/advisor_2020/#todo","text":"Add details taken from https://unnat.github.io/advisor/. Cite the arxiv paper. Give a list of things you can run with bash commands. Ideally be able to recreate a large set of experiments.","title":"TODO:"},{"location":"projects/babyai_baselines/","text":"Baseline experiments for the BabyAI environment # We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python ddmain.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp machine_params.gpu_id = 0 to the above command.","title":"BabyAI baselines"},{"location":"projects/babyai_baselines/#baseline-experiments-for-the-babyai-environment","text":"We perform a collection of baseline experiments within the BabyAI environment on the GoToLocal task, see the projects/babyai_baselines/experiments/go_to_local directory. For instance, to train a model using PPO, run python ddmain.py go_to_local.ppo --experiment_base projects/babyai_baselines/experiments Note that these experiments will be quite slow when not using a GPU as the BabyAI model architecture is surprisingly large. Specifying a GPU (if available) can be done from the command line using hooks we created using gin-config . E.g. to train using the 0th GPU device, add --gp machine_params.gpu_id = 0 to the above command.","title":"Baseline experiments for the BabyAI environment"},{"location":"projects/objectnav_baselines/","text":"Baseline models for the Object Navigation task in the RoboTHOR and iTHOR environments # TODO: # Add some details. Cite the RoboTHOR paper. Give a list of things you can run with bash commands","title":"ObjectNav baselines"},{"location":"projects/objectnav_baselines/#baseline-models-for-the-object-navigation-task-in-the-robothor-and-ithor-environments","text":"","title":"Baseline models for the Object Navigation task in the RoboTHOR and iTHOR environments"},{"location":"projects/objectnav_baselines/#todo","text":"Add some details. Cite the RoboTHOR paper. Give a list of things you can run with bash commands","title":"TODO:"},{"location":"projects/pointnav_baselines/","text":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments # TODO: # Add some details. Cite the Habitat, Navigation and RoboTHOR paper. Give a list of things you can run with bash commands.","title":"PointNav baselines"},{"location":"projects/pointnav_baselines/#baseline-models-for-the-point-navigation-task-in-the-habitat-robothor-and-ithor-environments","text":"","title":"Baseline models for the Point Navigation task in the Habitat, RoboTHOR and iTHOR environments"},{"location":"projects/pointnav_baselines/#todo","text":"Add some details. Cite the Habitat, Navigation and RoboTHOR paper. Give a list of things you can run with bash commands.","title":"TODO:"},{"location":"projects/two_body_problem_2019/","text":"Experiments for the Two Body Problem paper # TODO: # Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"Two Body Problem code"},{"location":"projects/two_body_problem_2019/#experiments-for-the-two-body-problem-paper","text":"","title":"Experiments for the Two Body Problem paper"},{"location":"projects/two_body_problem_2019/#todo","text":"Add details taken from https://prior.allenai.org/projects/two-body-problem Cite the CVPR paper. Give a list of things you can run with bash commands. At least a subset of the experiments.","title":"TODO:"},{"location":"tutorials/","text":"AllenAct Tutorials # We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework. Navigation in MiniGrid # We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here. PointNav in RoboTHOR # We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here. Swapping in a new environment # This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here.","title":"AllenAct Tutorials"},{"location":"tutorials/#allenact-tutorials","text":"We provide several tutorials to help ramp up researchers to the field of Embodied-AI as well as to the AllenAct framework.","title":"AllenAct Tutorials"},{"location":"tutorials/#navigation-in-minigrid","text":"We train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. This tutorial presents: Writing an experiment configuration file with a simple training pipeline from scratch. Using one of the supported environments with minimal user effort. Training, validation and testing your experiment from the command line. Follow the tutorial here.","title":"Navigation in MiniGrid"},{"location":"tutorials/#pointnav-in-robothor","text":"We train an agent on the Point Navigation task within the RoboTHOR Embodied-AI environment. This tutorial presents: The basics of the Point Navigation task, a common task in Embodied AI Using an external dataset Writing an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. Testing a pre-trained model Follow the tutorial here.","title":"PointNav in RoboTHOR"},{"location":"tutorials/#swapping-in-a-new-environment","text":"This tutorial demonstrates how easy it is modify the experiment config created in the RoboTHOR PointNav tutorial to work with the iTHOR and Habitat environments. Follow the tutorial here.","title":"Swapping in a new environment"},{"location":"tutorials/minigrid-tutorial/","text":"Tutorial: Navigation in MiniGrid # In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known. The task # A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls. Experiment configuration file # Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script ddmain.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc. Preliminaries # We first identify the experiment through a tag . class MiniGridTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) - str : return MiniGridTutorial Sensors and Model # A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in https://github.com/maximecb/gym-minigrid#wrappers. The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) - nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , ) Task samplers # We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) - TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = train ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = valid ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = test ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) - Dict [ str , Any ]: Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` if mode == train : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == test ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks )) deterministic_sampling = True # deterministically sample task in validation/testing return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation. Machine parameters # Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = train , ** kwargs ) - Dict [ str , Any ]: return { nprocesses : 128 if mode == train else 16 , gpu_ids : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of gpu_ids . Training pipeline # The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) - TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = Builder ( PPO , kwargs = {}, default = PPOConfig ,)), pipeline_stages = [ PipelineStage ( loss_names = [ ppo_loss ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( optim . Adam , dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { lr_lambda : LinearDecay ( steps = ppo_steps )} ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known. Training and validation # We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke python ddmain.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o /PATH/TO/minigrid_output we set the output folder. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to: Testing # The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to test for a specific experiment, we need to pass its training start date with the option -t EXPERIMENT_DATE : python ddmain.py minigrid_tutorial -b projects/tutorials -m 1 -o /PATH/TO/minigrid_output -s 12345 -t EXPERIMENT_DATE Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Navigation in Minigrid"},{"location":"tutorials/minigrid-tutorial/#tutorial-navigation-in-minigrid","text":"In this tutorial, we will train an agent to complete the MiniGrid-Empty-Random-5x5-v0 task within the MiniGrid environment. We will demonstrate how to: Write an experiment configuration file with a simple training pipeline from scratch. Use one of the supported environments with minimal user effort. Train, validate and test your experiment from the command line. This tutorial assumes the installation instructions have already been followed and, to some extent, this framework's abstractions are known.","title":"Tutorial: Navigation in MiniGrid"},{"location":"tutorials/minigrid-tutorial/#the-task","text":"A MiniGrid-Empty-Random-5x5-v0 task consists of a grid of dimensions 5x5 where an agent spawned at a random location and orientation has to navigate to the visitable bottom right corner cell of the grid by sequences of three possible actions (rotate left/right and move forward). A visualization of the environment with expert steps in a random MiniGrid-Empty-Random-5x5-v0 task looks like The observation for the agent is a subset of the entire grid, simulating a simplified limited field of view, as depicted by the highlighted rectangle (observed subset of the grid) around the agent (red arrow). Gray cells correspond to walls.","title":"The task"},{"location":"tutorials/minigrid-tutorial/#experiment-configuration-file","text":"Our complete experiment consists of: Training a basic actor-critic agent with memory to solve randomly sampled navigation tasks. Validation on a fixed set of tasks (running in parallel with training). A second stage where we test saved checkpoints with a larger fixed set of tasks. The entire configuration for the experiment, including training, validation, and testing, is encapsulated in a single class implementing the ExperimentConfig abstraction. For this tutorial, we will follow the config under projects/tutorials/minigrid_tutorial.py . The ExperimentConfig abstraction is used by the OnPolicyTrainer class (for training) and the OnPolicyInference class (for validation and testing) invoked through the entry script ddmain.py that calls an orchestrating OnPolicyRunner class. It includes: A tag method to identify the experiment. A create_model method to instantiate actor-critic models. A make_sampler_fn method to instantiate task samplers. Three {train,valid,test}_task_sampler_args methods describing initialization parameters for task samplers used in training, validation, and testing; including assignment of workers to devices for simulation. A machine_params method with configuration parameters that will be used for training, validation, and testing. A training_pipeline method describing a possibly multi-staged training pipeline with different types of losses, an optimizer, and other parameters like learning rates, batch sizes, etc.","title":"Experiment configuration file"},{"location":"tutorials/minigrid-tutorial/#preliminaries","text":"We first identify the experiment through a tag . class MiniGridTutorialExperimentConfig ( ExperimentConfig ): @classmethod def tag ( cls ) - str : return MiniGridTutorial","title":"Preliminaries"},{"location":"tutorials/minigrid-tutorial/#sensors-and-model","text":"A readily available Sensor type for MiniGrid, EgocentricMiniGridSensor , allows us to extract observations in a format consumable by an ActorCriticModel agent: SENSORS = [ EgocentricMiniGridSensor ( agent_view_size = 5 , view_channels = 3 ), ] The three view_channels include objects, colors and states corresponding to a partial observation of the environment as an image tensor, equivalent to that from ImgObsWrapper in https://github.com/maximecb/gym-minigrid#wrappers. The relatively large agent_view_size means the view will only be clipped by the environment walls in the forward and lateral directions with respect to the agent's orientation. We define our ActorCriticModel agent using a lightweight implementation with recurrent memory for MiniGrid environments, MiniGridSimpleConvRNN : @classmethod def create_model ( cls , ** kwargs ) - nn . Module : return MiniGridSimpleConvRNN ( action_space = gym . spaces . Discrete ( len ( MiniGridTask . class_action_names ())), observation_space = SensorSuite ( cls . SENSORS ) . observation_spaces , num_objects = cls . SENSORS [ 0 ] . num_objects , num_colors = cls . SENSORS [ 0 ] . num_colors , num_states = cls . SENSORS [ 0 ] . num_states , )","title":"Sensors and Model"},{"location":"tutorials/minigrid-tutorial/#task-samplers","text":"We use an available TaskSampler implementation for MiniGrid environments that allows to sample both random and deterministic MiniGridTasks , MiniGridTaskSampler : @classmethod def make_sampler_fn ( cls , ** kwargs ) - TaskSampler : return MiniGridTaskSampler ( ** kwargs ) This task sampler will during training (or validation/testing), randomly initialize new tasks for the agent to complete. While it is not quite as important for this task type (as we test our agent in the same setting it is trained on) there are a lot of good reasons we would like to sample tasks differently during training than during validation or testing. One good reason, that is applicable in this tutorial, is that, during training, we would like to be able to sample tasks forever while, during testing, we would like to sample a fixed number of tasks (as otherwise we would never finish testing!). In allenact this is made possible by defining different arguments for the task sampler: def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = train ) def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = valid ) def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: return self . _get_sampler_args ( process_ind = process_ind , mode = test ) where, for convenience, we have defined a _get_sampler_args method: def _get_sampler_args ( self , process_ind : int , mode : str ) - Dict [ str , Any ]: Generate initialization arguments for train, valid, and test TaskSamplers. # Parameters process_ind : index of the current task sampler mode: one of `train`, `valid`, or `test` if mode == train : max_tasks = None # infinite training tasks task_seeds_list = None # no predefined random seeds for training deterministic_sampling = False # randomly sample tasks in training else : max_tasks = 20 + 20 * ( mode == test ) # 20 tasks for valid, 40 for test # one seed for each task to sample: # - ensures different seeds for each sampler, and # - ensures a deterministic set of sampled tasks. task_seeds_list = list ( range ( process_ind * max_tasks , ( process_ind + 1 ) * max_tasks )) deterministic_sampling = True # deterministically sample task in validation/testing return dict ( max_tasks = max_tasks , # see above env_class = self . make_env , # builder for third-party environment (defined below) sensors = self . SENSORS , # sensors used to return observations to the agent env_info = dict (), # parameters for environment builder (none for now) task_seeds_list = task_seeds_list , # see above deterministic_sampling = deterministic_sampling # see above ) @staticmethod def make_env ( * args , ** kwargs ): return EmptyRandomEnv5x5 () Note that the env_class argument to the Task Sampler is the one determining which task type we are going to train the model for (in this case, MiniGrid-Empty-Random-5x5-v0 from gym-minigrid ) . The sparse reward is given by the environment , and the maximum task length is 100. For training, we opt for a default random sampling, whereas for validation and test we define fixed sets of randomly sampled tasks without needing to explicitly define a dataset. In this toy example, the maximum number of different tasks is 32. For validation we sample 320 tasks using 16 samplers, or 640 for testing, so we can be fairly sure that all possible tasks are visited at least once during evaluation.","title":"Task samplers"},{"location":"tutorials/minigrid-tutorial/#machine-parameters","text":"Given the simplicity of the task and model, we can quickly train the model on the CPU: @classmethod def machine_params ( cls , mode = train , ** kwargs ) - Dict [ str , Any ]: return { nprocesses : 128 if mode == train else 16 , gpu_ids : [], } We allocate a larger number of samplers for training (128) than for validation or testing (16), and we default to CPU usage by returning an empty list of gpu_ids .","title":"Machine parameters"},{"location":"tutorials/minigrid-tutorial/#training-pipeline","text":"The last definition required before starting to train is a training pipeline. In this case, we just use a single PPO stage with linearly decaying learning rate: @classmethod def training_pipeline ( cls , ** kwargs ) - TrainingPipeline : ppo_steps = int ( 150000 ) return TrainingPipeline ( named_losses = dict ( ppo_loss = Builder ( PPO , kwargs = {}, default = PPOConfig ,)), pipeline_stages = [ PipelineStage ( loss_names = [ ppo_loss ], max_stage_steps = ppo_steps ) ], optimizer_builder = Builder ( optim . Adam , dict ( lr = 1e-4 )), num_mini_batch = 4 , update_repeats = 3 , max_grad_norm = 0.5 , num_steps = 16 , gamma = 0.99 , use_gae = True , gae_lambda = 0.95 , advance_scene_rollout_period = None , save_interval = 10000 , metric_accumulate_interval = 1 , lr_scheduler_builder = Builder ( LambdaLR , { lr_lambda : LinearDecay ( steps = ppo_steps )} ), ) You can see that we use a Builder class to postpone the construction of some of the elements, like the optimizer, for which the model weights need to be known.","title":"Training pipeline"},{"location":"tutorials/minigrid-tutorial/#training-and-validation","text":"We have a complete implementation of this experiment's configuration class in projects/tutorials/minigrid_tutorial.py . To start training from scratch, we just need to invoke python ddmain.py minigrid_tutorial -b projects/tutorials -m 8 -o /PATH/TO/minigrid_output -s 12345 from the project root folder. With -b projects/tutorials we set the base folder to search for the minigrid_tutorial experiment configuration. With -m 8 we limit the number of subprocesses to 8 (each subprocess will run 16 of the 128 training task samplers). With -o /PATH/TO/minigrid_output we set the output folder. With -s 12345 we set the random seed. If we have Tensorboard installed, we can track progress with tensorboard --logdir /PATH/TO/minigrid_output which will default to the URL http://localhost:6006/ . After 150,000 steps, the script will terminate and several checkpoints will be saved in the output folder. The training curves should look similar to: If everything went well, the valid success rate should converge to 1 and the mean episode length to a value below 4. (For perfectly uniform sampling and complete observation, the expectation for the optimal policy is 3.75 steps.) In the not-so-unlikely event of the run failing to converge to a near-optimal policy, we can just try to re-run (for example with a different random seed). The validation curves should look similar to:","title":"Training and validation"},{"location":"tutorials/minigrid-tutorial/#testing","text":"The training start date for the experiment, in YYYY-MM-DD_HH-MM-SS format, is used as the name of one of the subfolders in the path to the checkpoints, saved under the output folder. In order to test for a specific experiment, we need to pass its training start date with the option -t EXPERIMENT_DATE : python ddmain.py minigrid_tutorial -b projects/tutorials -m 1 -o /PATH/TO/minigrid_output -s 12345 -t EXPERIMENT_DATE Again, if everything went well, the test success rate should converge to 1 and the mean episode length to a value below 4. Detailed results are saved under a metrics subfolder in the output folder. The test curves should look similar to:","title":"Testing"},{"location":"tutorials/running_inference_on_a_pretrained_model/","text":"Tutorial: Inference with a pre-trained model #","title":"Using a pre-trained model"},{"location":"tutorials/running_inference_on_a_pretrained_model/#tutorial-inference-with-a-pre-trained-model","text":"","title":"Tutorial: Inference with a pre-trained model"},{"location":"tutorials/training-a-pointnav-model/","text":"Tutorial: PointNav in RoboTHOR # Introduction # One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short. Pointnav # At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the house to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings. What is an environment anyways? # Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment. Learning algorithm # Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While allenact offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy (http://karpathy.github.io/2016/05/31/rl/), and this book by Sutton and Barto (http://www.incompleteideas.net/book/the-book-2nd.html). Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward. Dataset Setup # To train the model on the PointNav task, we need to download the dataset and precomputed cache of distances to the target. The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes. The precomputed cache of distances is a large dictionary containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance). We can download and unzip the data with the following commands: wget REDACTED unzip REDACTED Config File Setup # Now comes the most important part of the tutorial, we are going to write an experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called ddmain.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. We will start by creating a new directory inside the projects directory. We can name this whatever we want but for now, we will go with robothor_pointnav_tutuorial . Then we can create a directory called experiments inside the new directory we just created. This hierarchy is not necessary but it helps keep our experiments neatly organized. Now we create a file called pointnav_robothor_rgb_ddppo inside the experiments folder (again the name of this file is arbitrary). We start off by importing ExperimentConfig from the framework and defining a new subclass: from core.base_abstractions.experiment_config import ExperimentConfig class ObjectNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { step_penalty : - 0.01 , goal_success_reward : 10.0 , failed_stop_reward : 0.0 , shaping_weight : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. # Training Engine Parameters ADVANCE_SCENE_ROLLOUT_PERIOD = 10 ** 13 NUM_PROCESSES = 60 TRAINING_GPUS = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] VALIDATION_GPUS = [ 7 ] TESTING_GPUS = [ 7 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows # Dataset Parameters TRAIN_DATASET_DIR = dataset/robothor/objectnav/train VAL_DATASET_DIR = dataset/robothor/objectnav/val Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = rgb_lowres , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In allenact the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResnetPreProcessorHabitat , { input_height : SCREEN_SIZE , input_width : SCREEN_SIZE , output_width : 7 , output_height : 7 , output_dims : 512 , pool : False , torchvision_resnet_model : models . resnet18 , input_uuids : [ rgb_lowres ], output_uuid : rgb_resnet , parallel : False , # TODO False for debugging } ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ rgb_resnet , target_coordinates_ind , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return PointNavRobothorRGBPPO Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 metric_accumulate_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { ppo_loss : Builder ( PPO , kwargs = {}, default = PPOConfig , )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ ppo_loss ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { lr_lambda : LinearDecay ( steps = ppo_steps )} ), ) We define the helper method split_num_processes to split the different scenes that we want to train with amongst the different available devices. \"machine_params\" returns the hardware parameters of each process, based on the list of devices we defined above. def split_num_processes ( self , ndevices ): assert self . NUM_PROCESSES = ndevices , NUM_PROCESSES {} ndevices . format ( self . NUM_PROCESSES , ndevices ) res = [ 0 ] * ndevices for it in range ( self . NUM_PROCESSES ): res [ it % ndevices ] += 1 return res def machine_params ( self , mode = train , ** kwargs ): if mode == train : workers_per_device = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TRAINING_GPUS * workers_per_device nprocesses = 1 if not torch . cuda . is_available () else self . split_num_processes ( len ( gpu_ids )) sampler_devices = self . TRAINING_GPUS render_video = False elif mode == valid : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS render_video = False elif mode == test : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS render_video = False else : raise NotImplementedError ( mode must be train , valid , or test . ) # Disable parallelization for validation process if mode == valid : for prep in self . PREPROCESSORS : prep . kwargs [ parallel ] = False observation_set = Builder ( ObservationSet , kwargs = dict ( source_ids = self . OBSERVATIONS , all_preprocessors = self . PREPROCESSORS , all_sensors = self . SENSORS )) if mode == train or nprocesses 0 else None return { nprocesses : nprocesses , gpu_ids : gpu_ids , sampler_devices : sampler_devices if mode == train else gpu_ids , observation_set : observation_set , render_video : render_video , } Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a model from the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . # Define Model @classmethod def create_model ( cls , ** kwargs ) - nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ observation_set ] . observation_spaces , goal_sensor_uuid = target_coordinates_ind , rgb_resnet_preprocessor_uuid = rgb_resnet , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. # Define Task Sampler @classmethod def make_sampler_fn ( cls , ** kwargs ) - TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. # Utility Functions for distributing scenes between GPUs @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: path = scenes_dir + *.json.gz if scenes_dir [ - 1 ] == / else scenes_dir + /*.json.gz scenes = [ scene . split ( / )[ - 1 ] . split ( . )[ 0 ] for scene in glob . glob ( path )] if total_processes len ( scenes ): # oversample some scenes - bias if total_processes % len ( scenes ) != 0 : print ( Warning: oversampling some of the scenes to feed all processes. You can avoid this by setting a number of workers divisible by the number of scenes ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( Warning: oversampling some of the scenes to feed all processes. You can avoid this by setting a number of workers divisor of the number of scenes ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { scenes : scenes [ inds [ process_ind ]: inds [ process_ind + 1 ]], max_steps : self . MAX_STEPS , sensors : self . SENSORS , action_space : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), seed : seeds [ process_ind ] if seeds is not None else None , deterministic_cudnn : deterministic_cudnn , rewards_config : self . REWARD_CONFIG } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_DATASET_DIR + /episodes/ , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ scene_directory ] = self . TRAIN_DATASET_DIR res [ loop_dataset ] = True res [ env_args ] = {} res [ env_args ] . update ( self . ENV_ARGS ) res [ env_args ][ x_display ] = ( ( 0. %d % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) 0 else None ) res [ allow_flipping ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + /episodes/ , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ scene_directory ] = self . VAL_DATASET_DIR res [ loop_dataset ] = False res [ env_args ] = {} res [ env_args ] . update ( self . ENV_ARGS ) res [ env_args ][ x_display ] = ( ( 0. %d % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + /episodes/ , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ scene_directory ] = self . VAL_DATASET_DIR res [ loop_dataset ] = False res [ env_args ] = {} res [ env_args ] . update ( self . ENV_ARGS ) res [ env_args ][ x_display ] = 10.0 return res This is it! If we copy all of the code into a file we should be able to run our experiment! Testing Pre-Trained Model # With the experiment all set up, we can try testing it with pre-trained weights. We can download and unzip these weights with the following commands: mkdir projects/pointnav_robothor_rgb/weights cd projects/pointnav_robothor_rgb/weights cd models wget REDACTED unzip REDACTED We can then test the model by running: python ddmain.py -o PATH_TO_OUTPUT -c PATH_TO_CHECKPOINT -t -b BASE_DIRECTORY_OF_YOUR_EXPERIMENT EXPERIMENT_NAME Where PATH_TO_OUTPUT is the location where the results of the test will be dumped, PATH_TO_CHECKPOINT is the location of the downloaded model weights, BASE_DIRECTORY_OF_YOUR_EXPERIMENT is a path to the directory where our experiment definition is stored and is simply the name of our experiment (without the file extension). For our current setup the following command would work: python ddmain.py -o projects/pointnav_robothor_rgb/storage/ -c projects/pointnav_robothor_rgb/weights/NAME -t -b projects/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo The scripts should produce a json output in the specified folder containing the results of our test. Training Model From Scratch # We can also train the model from scratch by running: python ddmain.py -o PATH_TO_OUTPUT -c -b BASE_DIRECTORY_OF_YOUR_EXPERIMENT EXPERIMENT_NAME But be aware, training this takes nearly 2 days on a machine with 8 GPU. For our current setup the following command would work: python ddmain.py -o projects/pointnav_robothor_rgb/storage/ -b projects/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this: Conclusion # In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#tutorial-pointnav-in-robothor","text":"","title":"Tutorial: PointNav in RoboTHOR"},{"location":"tutorials/training-a-pointnav-model/#introduction","text":"One of the most obvious tasks that an embodied agent should master is navigating the world it inhabits. Before we can teach a robot to cook or clean it first needs to be able to move around. The simplest way to formulate \"moving around\" into a task is by making your agent find a beacon somewhere in the environment. This beacon transmits its location, such that at any time, the agent can get the direction and euclidian distance to the beacon. This particular task is often called Point Navigation, or PointNav for short.","title":"Introduction"},{"location":"tutorials/training-a-pointnav-model/#pointnav","text":"At first glance, this task seems trivial. If the agent is given the direction and distance of the target at all times, can it not simply follow this signal directly? The answer is no, because agents are often trained on this task in environments that emulate real-world buildings which are not wide-open spaces, but rather contain many smaller rooms. Because of this, the agent has to learn to navigate human spaces and use doors and hallways to efficiently navigate from one side of the house to the other. This task becomes particularly difficult when the agent is tested in an environment that it is not trained in. If the agent does not know how the floor plan of an environment looks, it has to learn to predict the design of man-made structures, to efficiently navigate across them, much like how people instinctively know how to move around a building they have never seen before based on their experience navigating similar buildings.","title":"Pointnav"},{"location":"tutorials/training-a-pointnav-model/#what-is-an-environment-anyways","text":"Environments are worlds in which embodied agents exist. If our embodied agent is simply a neural network that is being trained in a simulator, then that simulator is its environment. Similarly, if our agent is a physical robot then its environment is the real world. The agent interacts with the environment by taking one of several available actions (such as \"move forward\", or \"turn left\"). After each action, the environment produces a new frame that the agent can analyze to determine its next step. For many tasks, including PointNav the agent also has a special \"stop\" action which indicates that the agent thinks it has reached the target. After this action is called the agent will be reset to a new location, regardless if it reached the target. The hope is that after enough training the agent will learn to correctly assess that it has successfully navigated to the target. There are many simulators designed for the training of embodied agents. In this tutorial, we will be using a simulator called RoboTHOR , which is designed specifically to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment that researchers can have access to. RoboTHOR contains 60 different virtual scenes with different floor plans and furniture and 15 validation scenes. It is also important to mention that AllenAct has a class abstraction called Environment. This is not the actual simulator game engine or robotics controller, but rather a shallow wrapper that provides a uniform interface to the actual environment.","title":"What is an environment anyways?"},{"location":"tutorials/training-a-pointnav-model/#learning-algorithm","text":"Finally, let us briefly touch on the algorithm that we will use to train our embodied agent to navigate. While allenact offers us great flexibility to train models using complex pipelines, we will be using a simple pure reinforcement learning approach for this tutorial. More specifically, we will be using DD-PPO, a decentralized and distributed variant of the ubiquitous PPO algorithm. For those unfamiliar with Reinforcement Learning we highly recommend this tutorial by Andrej Karpathy (http://karpathy.github.io/2016/05/31/rl/), and this book by Sutton and Barto (http://www.incompleteideas.net/book/the-book-2nd.html). Essentially what we are doing is letting our agent explore the environment on its own, rewarding it for taking actions that bring it closer to its goal and penalizing it for actions that take it away from its goal. We then optimize the agent's model to maximize this reward.","title":"Learning algorithm"},{"location":"tutorials/training-a-pointnav-model/#dataset-setup","text":"To train the model on the PointNav task, we need to download the dataset and precomputed cache of distances to the target. The dataset contains a list of episodes with thousands of randomly generated starting positions and target locations for each of the scenes. The precomputed cache of distances is a large dictionary containing the shortest path from each point in a scene, to every other point in that scene. This is used to reward the agent for moving closer to the target in terms of geodesic distance - the actual path distance (as opposed to a straight line distance). We can download and unzip the data with the following commands: wget REDACTED unzip REDACTED","title":"Dataset Setup"},{"location":"tutorials/training-a-pointnav-model/#config-file-setup","text":"Now comes the most important part of the tutorial, we are going to write an experiment config file. Unlike a library that can be imported into python, AllenAct is structured as a framework with a runner script called ddmain.py which will run the experiment specified in a config file. This design forces us to keep meticulous records of exactly which settings were used to produce a particular result, which can be very useful given how expensive RL models are to train. We will start by creating a new directory inside the projects directory. We can name this whatever we want but for now, we will go with robothor_pointnav_tutuorial . Then we can create a directory called experiments inside the new directory we just created. This hierarchy is not necessary but it helps keep our experiments neatly organized. Now we create a file called pointnav_robothor_rgb_ddppo inside the experiments folder (again the name of this file is arbitrary). We start off by importing ExperimentConfig from the framework and defining a new subclass: from core.base_abstractions.experiment_config import ExperimentConfig class ObjectNavRoboThorRGBPPOExperimentConfig ( ExperimentConfig ): We then define the task parameters. For PointNav, these include the maximum number of steps our agent can take before being reset (this prevents the agent from wandering on forever), and a configuration for the reward function that we will be using. # Task Parameters MAX_STEPS = 500 REWARD_CONFIG = { step_penalty : - 0.01 , goal_success_reward : 10.0 , failed_stop_reward : 0.0 , shaping_weight : 1.0 , } In this case, we set the maximum number of steps to 500. We give the agent a reward of -0.01 for each action that it takes (this is to encourage it to reach the goal in as few actions as possible), and a reward of 10.0 if the agent manages to successfully reach its destination. If the agent selects the stop action without reaching the target we do not punish it (although this is sometimes useful for preventing the agent from stopping prematurely). Finally, our agent gets rewarded if it moves closer to the target and gets punished if it moves further away. shaping_weight controls how strong this signal should be and is here set to 1.0. These parameters work well for training an agent on PointNav, but feel free to play around with them. Next, we set the parameters of the simulator itself. Here we select a resolution at which the engine will render every frame (640 by 480) and a resolution at which the image will be fed into the neural network (here it is set to a 224 by 224 box). # Simulator Parameters CAMERA_WIDTH = 640 CAMERA_HEIGHT = 480 SCREEN_SIZE = 224 Next, we set the hardware parameters for the training engine. NUM_PROCESSES sets the total number of parallel processes that will be used to train the model. In general, more processes result in faster training, but since each process is a unique instance of the environment in which we are training they can take up a lot of memory. Depending on the size of the model, the environment, and the hardware we are using, we may need to adjust this number, but for a setup with 8 GTX Titans, 60 processes work fine. 60 also happens to be the number of training scenes in RoboTHOR, which allows each process to load only a single scene into memory, saving time and space. TRAINING_GPUS takes the ids of the GPUS on which the model should be trained. Similarly VALIDATION_GPUS and TESTING_GPUS hold the ids of the GPUS on which the validation and testing will occur. During training, a validation process is constantly running and evaluating the current model, to show the progress on the validation set, so reserving a GPU for validation can be a good idea. If our hardware setup does not include a GPU, these fields can be set to empty lists, as the codebase will default to running everything on the CPU with only 1 process. # Training Engine Parameters ADVANCE_SCENE_ROLLOUT_PERIOD = 10 ** 13 NUM_PROCESSES = 60 TRAINING_GPUS = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] VALIDATION_GPUS = [ 7 ] TESTING_GPUS = [ 7 ] Since we are using a dataset to train our model we need to define the path to where we have stored it. If we download the dataset instructed above we can define the path as follows # Dataset Parameters TRAIN_DATASET_DIR = dataset/robothor/objectnav/train VAL_DATASET_DIR = dataset/robothor/objectnav/val Next, we define the sensors. RGBSensorThor is the environment's implementation of an RGB sensor. It takes the raw image outputted by the simulator and resizes it, to the input dimensions for our neural network that we specified above. It also performs normalization if we want. GPSCompassSensorRoboThor is a sensor that tracks the point our agent needs to move to. It tells us the direction and distance to our goal at every time step. SENSORS = [ RGBSensorThor ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , uuid = rgb_lowres , ), GPSCompassSensorRoboThor (), ] For the sake of this example, we are also going to be using a preprocessor with our model. In allenact the preprocessor abstraction is designed with large models with frozen weights in mind. These models often hail from the ResNet family and transform the raw pixels that our agent observes in the environment, into a complex embedding, which then gets stored and used as input to our trainable model instead of the original image. Most other preprocessing work is done in the sensor classes (as we just saw with the RGB sensor scaling and normalizing our input), but for the sake of efficiency, all neural network preprocessing should use this abstraction. PREPROCESSORS = [ Builder ( ResnetPreProcessorHabitat , { input_height : SCREEN_SIZE , input_width : SCREEN_SIZE , output_width : 7 , output_height : 7 , output_dims : 512 , pool : False , torchvision_resnet_model : models . resnet18 , input_uuids : [ rgb_lowres ], output_uuid : rgb_resnet , parallel : False , # TODO False for debugging } ), ] Next, we must define all of the observation inputs that our model will use. These are just the hardcoded ids of the sensors we are using in the experiment. OBSERVATIONS = [ rgb_resnet , target_coordinates_ind , ] Finally, we must define the settings of our simulator. We set the camera dimensions to the values we defined earlier. We set rotateStepDegrees to 30 degrees, which means that every time the agent takes a turn action, they will rotate by 30 degrees. We set grid size to 0.25 which means that every time the agent moves forward, it will do so by 0.25 meters. ENV_ARGS = dict ( width = CAMERA_WIDTH , height = CAMERA_HEIGHT , rotateStepDegrees = 30.0 , gridSize = 0.25 , ) Now we move on to the methods that we must define to finish implementing an experiment config. Firstly we have a simple method that just returns the name of the experiment. @classmethod def tag ( cls ): return PointNavRobothorRGBPPO Next, we define the training pipeline. In this function, we specify exactly which algorithm or algorithms we will use to train our model. In this simple example, we are using the PPO loss with a learning rate of 3e-4. We specify 250 million steps of training and a rollout length of 30 with the ppo_steps and num_steps parameters respectively. All the other standard PPO parameters are also present in this function. metric_accumulate_interval sets the frequency at which data is accumulated from all the processes and logged while save_interval sets how often we save the model weights and run validation on them. @classmethod def training_pipeline ( cls , ** kwargs ): ppo_steps = int ( 250000000 ) lr = 3e-4 num_mini_batch = 1 update_repeats = 3 num_steps = 30 save_interval = 5000000 metric_accumulate_interval = 10000 gamma = 0.99 use_gae = True gae_lambda = 0.95 max_grad_norm = 0.5 return TrainingPipeline ( save_interval = save_interval , metric_accumulate_interval = metric_accumulate_interval , optimizer_builder = Builder ( optim . Adam , dict ( lr = lr )), num_mini_batch = num_mini_batch , update_repeats = update_repeats , max_grad_norm = max_grad_norm , num_steps = num_steps , named_losses = { ppo_loss : Builder ( PPO , kwargs = {}, default = PPOConfig , )}, gamma = gamma , use_gae = use_gae , gae_lambda = gae_lambda , advance_scene_rollout_period = cls . ADVANCE_SCENE_ROLLOUT_PERIOD , pipeline_stages = [ PipelineStage ( loss_names = [ ppo_loss ], max_stage_steps = ppo_steps ) ], lr_scheduler_builder = Builder ( LambdaLR , { lr_lambda : LinearDecay ( steps = ppo_steps )} ), ) We define the helper method split_num_processes to split the different scenes that we want to train with amongst the different available devices. \"machine_params\" returns the hardware parameters of each process, based on the list of devices we defined above. def split_num_processes ( self , ndevices ): assert self . NUM_PROCESSES = ndevices , NUM_PROCESSES {} ndevices . format ( self . NUM_PROCESSES , ndevices ) res = [ 0 ] * ndevices for it in range ( self . NUM_PROCESSES ): res [ it % ndevices ] += 1 return res def machine_params ( self , mode = train , ** kwargs ): if mode == train : workers_per_device = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TRAINING_GPUS * workers_per_device nprocesses = 1 if not torch . cuda . is_available () else self . split_num_processes ( len ( gpu_ids )) sampler_devices = self . TRAINING_GPUS render_video = False elif mode == valid : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . VALIDATION_GPUS render_video = False elif mode == test : nprocesses = 1 gpu_ids = [] if not torch . cuda . is_available () else self . TESTING_GPUS render_video = False else : raise NotImplementedError ( mode must be train , valid , or test . ) # Disable parallelization for validation process if mode == valid : for prep in self . PREPROCESSORS : prep . kwargs [ parallel ] = False observation_set = Builder ( ObservationSet , kwargs = dict ( source_ids = self . OBSERVATIONS , all_preprocessors = self . PREPROCESSORS , all_sensors = self . SENSORS )) if mode == train or nprocesses 0 else None return { nprocesses : nprocesses , gpu_ids : gpu_ids , sampler_devices : sampler_devices if mode == train else gpu_ids , observation_set : observation_set , render_video : render_video , } Now we define the actual model that we will be using. AllenAct offers first-class support for PyTorch, so any PyTorch model that implements the provided ActorCriticModel class will work here. Here we borrow a model from the pointnav_baselines project (which unsurprisingly contains several PointNav baselines). It is a small convolutional network that expects the output of a ResNet as its rgb input followed by a single-layered GRU. The model accepts as input the number of different actions our agent can perform in the environment through the action_space parameter, which we get from the task definition. We also define the shape of the inputs we are going to be passing to the model with observation_space We specify the names of our sensors with goal_sensor_uuid and rgb_resnet_preprocessor_uuid . Finally, we define the size of our RNN with hidden_layer and the size of the embedding of our goal sensor data (the direction and distance to the target) with goal_dims . # Define Model @classmethod def create_model ( cls , ** kwargs ) - nn . Module : return ResnetTensorPointNavActorCritic ( action_space = gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), observation_space = kwargs [ observation_set ] . observation_spaces , goal_sensor_uuid = target_coordinates_ind , rgb_resnet_preprocessor_uuid = rgb_resnet , hidden_size = 512 , goal_dims = 32 , ) We also need to define the task sampler that we will be using. This is a piece of code that generates instances of tasks for our agent to perform (essentially starting locations and targets for PointNav). Since we are getting our tasks from a dataset, the task sampler is a very simple code that just reads the specified file and sets the agent to the next starting locations whenever the agent exceeds the maximum number of steps or selects the stop action. # Define Task Sampler @classmethod def make_sampler_fn ( cls , ** kwargs ) - TaskSampler : return PointNavDatasetTaskSampler ( ** kwargs ) You might notice that we did not specify the task sampler's arguments, but are rather passing them in. The reason for this is that each process will have its own task sampler, and we need to specify exactly which scenes each process should work with. If we have several GPUS and many scenes this process of distributing the work can be rather complicated so we define a few helper functions to do just this. # Utility Functions for distributing scenes between GPUs @staticmethod def _partition_inds ( n : int , num_parts : int ): return np . round ( np . linspace ( 0 , n , num_parts + 1 , endpoint = True )) . astype ( np . int32 ) def _get_sampler_args_for_scene_split ( self , scenes_dir : str , process_ind : int , total_processes : int , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: path = scenes_dir + *.json.gz if scenes_dir [ - 1 ] == / else scenes_dir + /*.json.gz scenes = [ scene . split ( / )[ - 1 ] . split ( . )[ 0 ] for scene in glob . glob ( path )] if total_processes len ( scenes ): # oversample some scenes - bias if total_processes % len ( scenes ) != 0 : print ( Warning: oversampling some of the scenes to feed all processes. You can avoid this by setting a number of workers divisible by the number of scenes ) scenes = scenes * int ( ceil ( total_processes / len ( scenes ))) scenes = scenes [: total_processes * ( len ( scenes ) // total_processes )] else : if len ( scenes ) % total_processes != 0 : print ( Warning: oversampling some of the scenes to feed all processes. You can avoid this by setting a number of workers divisor of the number of scenes ) inds = self . _partition_inds ( len ( scenes ), total_processes ) return { scenes : scenes [ inds [ process_ind ]: inds [ process_ind + 1 ]], max_steps : self . MAX_STEPS , sensors : self . SENSORS , action_space : gym . spaces . Discrete ( len ( PointNavTask . class_action_names ())), seed : seeds [ process_ind ] if seeds is not None else None , deterministic_cudnn : deterministic_cudnn , rewards_config : self . REWARD_CONFIG } The very last things we need to define are the sampler arguments themselves. We define them separately for a train, validation, and test sampler, but in this case, they are almost the same. The arguments need to include the location of the dataset and distance cache as well as the environment arguments for our simulator, both of which we defined above and are just referencing here. The only consequential differences between these task samplers are the path to the dataset we are using (train or validation) and whether we want to loop over the dataset or not (we want this for training since we want to train for several epochs, but we do not need this for validation and testing). Since the test scenes of RoboTHOR are private we are also testing on our validation set. def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . TRAIN_DATASET_DIR + /episodes/ , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ scene_directory ] = self . TRAIN_DATASET_DIR res [ loop_dataset ] = True res [ env_args ] = {} res [ env_args ] . update ( self . ENV_ARGS ) res [ env_args ][ x_display ] = ( ( 0. %d % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) 0 else None ) res [ allow_flipping ] = True return res def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + /episodes/ , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ scene_directory ] = self . VAL_DATASET_DIR res [ loop_dataset ] = False res [ env_args ] = {} res [ env_args ] . update ( self . ENV_ARGS ) res [ env_args ][ x_display ] = ( ( 0. %d % devices [ process_ind % len ( devices )]) if devices is not None and len ( devices ) 0 else None ) return res def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: res = self . _get_sampler_args_for_scene_split ( self . VAL_DATASET_DIR + /episodes/ , process_ind , total_processes , seeds = seeds , deterministic_cudnn = deterministic_cudnn , ) res [ scene_directory ] = self . VAL_DATASET_DIR res [ loop_dataset ] = False res [ env_args ] = {} res [ env_args ] . update ( self . ENV_ARGS ) res [ env_args ][ x_display ] = 10.0 return res This is it! If we copy all of the code into a file we should be able to run our experiment!","title":"Config File Setup"},{"location":"tutorials/training-a-pointnav-model/#testing-pre-trained-model","text":"With the experiment all set up, we can try testing it with pre-trained weights. We can download and unzip these weights with the following commands: mkdir projects/pointnav_robothor_rgb/weights cd projects/pointnav_robothor_rgb/weights cd models wget REDACTED unzip REDACTED We can then test the model by running: python ddmain.py -o PATH_TO_OUTPUT -c PATH_TO_CHECKPOINT -t -b BASE_DIRECTORY_OF_YOUR_EXPERIMENT EXPERIMENT_NAME Where PATH_TO_OUTPUT is the location where the results of the test will be dumped, PATH_TO_CHECKPOINT is the location of the downloaded model weights, BASE_DIRECTORY_OF_YOUR_EXPERIMENT is a path to the directory where our experiment definition is stored and is simply the name of our experiment (without the file extension). For our current setup the following command would work: python ddmain.py -o projects/pointnav_robothor_rgb/storage/ -c projects/pointnav_robothor_rgb/weights/NAME -t -b projects/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo The scripts should produce a json output in the specified folder containing the results of our test.","title":"Testing Pre-Trained Model"},{"location":"tutorials/training-a-pointnav-model/#training-model-from-scratch","text":"We can also train the model from scratch by running: python ddmain.py -o PATH_TO_OUTPUT -c -b BASE_DIRECTORY_OF_YOUR_EXPERIMENT EXPERIMENT_NAME But be aware, training this takes nearly 2 days on a machine with 8 GPU. For our current setup the following command would work: python ddmain.py -o projects/pointnav_robothor_rgb/storage/ -b projects/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo If we start up a tensorboard server during training and specify that output_dir=storage the output should look something like this:","title":"Training Model From Scratch"},{"location":"tutorials/training-a-pointnav-model/#conclusion","text":"In this tutorial, we learned how to create a new PointNav experiment using AllenAct . There are many simple and obvious ways to modify the experiment from here - changing the model, the learning algorithm and the environment each requires very few lines of code changed in the above file, allowing us to explore our embodied ai research ideas across different frameworks with ease.","title":"Conclusion"},{"location":"tutorials/training-pipelines/","text":"Tutorial: IL to RL with a training pipeline #","title":"IL to RL with pipelines"},{"location":"tutorials/training-pipelines/#tutorial-il-to-rl-with-a-training-pipeline","text":"","title":"Tutorial: IL to RL with a training pipeline"},{"location":"tutorials/transfering-to-a-different-environment-framework/","text":"Tutorial: Swapping in a new environment # Introduction # This tutorial was designed as a continuation of the Robothor Pointnav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another. RoboTHOR to iTHOR # Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = dataset/ithor/objectnav/train VAL_DATASET_DIR = dataset/ithor/objectnav/val That's it! We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return PointNavRobothorRGBPPO RoboTHOR to Habitat # Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = habitat . get_config ( configs/gibson.yaml ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = habitat/habitat-api/data/scene_datasets/ CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ * ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ RGB_SENSOR ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = Nav-v0 CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ POINTGOAL_WITH_GPS_COMPASS_SENSOR ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = POLAR CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = pointgoal_with_gps_compass CONFIG . TASK . MEASUREMENTS = [ DISTANCE_TO_GOAL , SPL ] CONFIG . TASK . SPL . TYPE = SPL CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = train This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) - TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: config = self . TRAIN_CONFIGS [ process_ind ] return { env_config : config , max_steps : self . MAX_STEPS , sensors : self . SENSORS , action_space : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), distance_to_goal : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES config . MODE = validate config . freeze () return { env_config : config , max_steps : self . MAX_STEPS , sensors : self . SENSORS , action_space : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), distance_to_goal : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { env_config : config , max_steps : self . MAX_STEPS , sensors : self . SENSORS , action_space : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), distance_to_goal : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters. Running a Test # With the setup complete, we should be able to run a test using the exact same command as in the last tutorial: python ddmain.py -o projects/pointnav_transfer_turotial/ -c projects/pointnav_robothor_rgb/weights/ REDACTED -t -b projects/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo This should test the model trained in RoboTHOR on either iTHOR or Habitat (depending on which modifications we made). Conclusion # In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Swapping environments"},{"location":"tutorials/transfering-to-a-different-environment-framework/#tutorial-swapping-in-a-new-environment","text":"","title":"Tutorial: Swapping in a new environment"},{"location":"tutorials/transfering-to-a-different-environment-framework/#introduction","text":"This tutorial was designed as a continuation of the Robothor Pointnav Tutorial and explains how to modify the experiment config created in that tutorial to work with the iTHOR and Habitat environments. Cross-platform support is one of the key design goals of allenact . This is achieved through a total decoupling of the environment code from the engine, model and algorithm code, so that swapping in a new environment is as plug and play as possible. Crucially we will be able to run a model on different environments without touching the model code at all, which will allow us to train neural networks in one environment and test them in another.","title":"Introduction"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-ithor","text":"Since both the RoboTHOR and the iTHOR environment stem from the same family and are developed by the same organization, switching between the two is incredibly easy. We only have to change the path parameter to point to an iTHOR dataset rather than the RoboTHOR one. # Dataset Parameters TRAIN_DATASET_DIR = dataset/ithor/objectnav/train VAL_DATASET_DIR = dataset/ithor/objectnav/val That's it! We might also want to modify the tag method to accurately reflect our config but this will not change the behavior at all and is merely a bookkeeping convenience. @classmethod def tag ( cls ): return PointNavRobothorRGBPPO","title":"RoboTHOR to iTHOR"},{"location":"tutorials/transfering-to-a-different-environment-framework/#robothor-to-habitat","text":"Since the roboTHOR and Habitat simulators are sufficiently different and have different parameters to configure this transformation takes a bit more effort, but we only need to modify the environment config and TaskSampler (we have to change the former because the habitat simulator accepts a different format of configuration and the latter because the habitat dataset is formatted differently and thus needs to be parsed differently.) As part of our environment modification, we need to switch from using RoboTHOR sensors to using Habitat sensors. The implementation of sensors we provide offer an uniform interface across all the environments so we simply have to swap out our sensor classes: SENSORS = [ DepthSensorHabitat ( height = SCREEN_SIZE , width = SCREEN_SIZE , use_resnet_normalization = True , ), TargetCoordinatesSensorHabitat ( coordinate_dims = 2 ), ] Next we need to define the simulator config: CONFIG = habitat . get_config ( configs/gibson.yaml ) CONFIG . defrost () CONFIG . NUM_PROCESSES = NUM_PROCESSES CONFIG . SIMULATOR_GPU_IDS = TRAIN_GPUS CONFIG . DATASET . SCENES_DIR = habitat/habitat-api/data/scene_datasets/ CONFIG . DATASET . POINTNAVV1 . CONTENT_SCENES = [ * ] CONFIG . DATASET . DATA_PATH = TRAIN_SCENES CONFIG . SIMULATOR . AGENT_0 . SENSORS = [ RGB_SENSOR ] CONFIG . SIMULATOR . RGB_SENSOR . WIDTH = CAMERA_WIDTH CONFIG . SIMULATOR . RGB_SENSOR . HEIGHT = CAMERA_HEIGHT CONFIG . SIMULATOR . TURN_ANGLE = 30 CONFIG . SIMULATOR . FORWARD_STEP_SIZE = 0.25 CONFIG . ENVIRONMENT . MAX_EPISODE_STEPS = MAX_STEPS CONFIG . TASK . TYPE = Nav-v0 CONFIG . TASK . SUCCESS_DISTANCE = 0.2 CONFIG . TASK . SENSORS = [ POINTGOAL_WITH_GPS_COMPASS_SENSOR ] CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . GOAL_FORMAT = POLAR CONFIG . TASK . POINTGOAL_WITH_GPS_COMPASS_SENSOR . DIMENSIONALITY = 2 CONFIG . TASK . GOAL_SENSOR_UUID = pointgoal_with_gps_compass CONFIG . TASK . MEASUREMENTS = [ DISTANCE_TO_GOAL , SPL ] CONFIG . TASK . SPL . TYPE = SPL CONFIG . TASK . SPL . SUCCESS_DISTANCE = 0.2 CONFIG . MODE = train This CONFIG object holds very similar values to the ones ENV_ARGS held in the RoboTHOR example. We decided to leave this way of passing in configurations exposed to the user to offer maximum customization of the underlying environment. Finally we need to replace the task sampler and its argument generating functions: # Define Task Sampler from plugins.habitat_plugin.habitat_task_samplers import PointNavTaskSampler @classmethod def make_sampler_fn ( cls , ** kwargs ) - TaskSampler : return PointNavTaskSampler ( ** kwargs ) def train_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: config = self . TRAIN_CONFIGS [ process_ind ] return { env_config : config , max_steps : self . MAX_STEPS , sensors : self . SENSORS , action_space : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), distance_to_goal : self . DISTANCE_TO_GOAL , } def valid_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: config = self . CONFIG . clone () config . defrost () config . DATASET . DATA_PATH = self . VALID_SCENES config . MODE = validate config . freeze () return { env_config : config , max_steps : self . MAX_STEPS , sensors : self . SENSORS , action_space : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), distance_to_goal : self . DISTANCE_TO_GOAL , } def test_task_sampler_args ( self , process_ind : int , total_processes : int , devices : Optional [ List [ int ]] = None , seeds : Optional [ List [ int ]] = None , deterministic_cudnn : bool = False , ) - Dict [ str , Any ]: config = self . TEST_CONFIGS [ process_ind ] return { env_config : config , max_steps : self . MAX_STEPS , sensors : self . SENSORS , action_space : gym . spaces . Discrete ( len ( PointNavTask . action_names ())), distance_to_goal : self . DISTANCE_TO_GOAL , } As we can see this code looks very similar as well, we simply need to pass slightly different parameters.","title":"RoboTHOR to Habitat"},{"location":"tutorials/transfering-to-a-different-environment-framework/#running-a-test","text":"With the setup complete, we should be able to run a test using the exact same command as in the last tutorial: python ddmain.py -o projects/pointnav_transfer_turotial/ -c projects/pointnav_robothor_rgb/weights/ REDACTED -t -b projects/pointnav_robothor_rgb/experiments pointnav_robothor_rgb_ddppo This should test the model trained in RoboTHOR on either iTHOR or Habitat (depending on which modifications we made).","title":"Running a Test"},{"location":"tutorials/transfering-to-a-different-environment-framework/#conclusion","text":"In this tutorial, we learned how to modify our experiment configurations to work with different environments. By providing a high level of modularity and out-of-the-box support for both Habitat and THOR , two of the most popular embodied frameworks out there AllenAct hopes to give researchers the ability to validate their results across many platforms and help guide them towards genuine progress. The source code for this tutorial can be found in /projects/framework_transfer_tutorial .","title":"Conclusion"}]}